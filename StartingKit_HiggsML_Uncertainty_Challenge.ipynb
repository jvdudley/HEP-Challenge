{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Kit - FAIR UNIVERSE: HIGGSML UNCERTAINTY CHALLENGE\n",
    "\n",
    "For Overview and Decsiptions of the competition, please visit the competition page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "`COLAB` determines whether this notebook is running on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # clone github repo\n",
    "    !git clone https://github.com/FAIR-Universe/HEP-Challenge.git\n",
    "    !pip install iminuit\n",
    "\n",
    "    # move to the HEP starting kit folder\n",
    "    %cd HEP-Challenge/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SETTINGS = {\n",
    "\"systematics\": {  # Systematics to use\n",
    "    \"tes\": False, # tau energy scale\n",
    "    \"jes\": False, # jet energy scale\n",
    "    \"soft_met\": False, # soft term in MET\n",
    "    \"ttbar_scale\": False, # ttbar scale factor\n",
    "    \"diboson_scale\": False, # diboson scale factor\n",
    "    \"bkg_scale\": False, # Background scale factor\n",
    "    },\n",
    "\"num_pseudo_experiments\" : 100 , # Number of pseudo-experiments to run per set\n",
    "\"num_of_sets\" : 1, # Number of sets of pseudo-experiments to run\n",
    "} \n",
    "\n",
    "USE_RANDOM_MUS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Submissions\n",
    "By this point you should have a clone of the repo which contains `HiggsML_Dummy_Submission.zip` which you can submit to the Competition\n",
    "\n",
    "For more sample submissions please check `/HEP-Challenge/example_submissions/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from numpy.random import RandomState\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_dir = os.getcwd()\n",
    "print(\"Root directory is\", root_dir)\n",
    "\n",
    "input_dir = os.path.join(root_dir, \"input_data\")\n",
    "output_dir = os.path.join(root_dir, \"sample_result_submission\")\n",
    "submission_dir = os.path.join(root_dir, \"sample_code_submission\")\n",
    "program_dir = os.path.join(root_dir, \"ingestion_program\")\n",
    "score_dir = os.path.join(root_dir, \"scoring_program\")\n",
    "    \n",
    "test_settings = TEST_SETTINGS.copy()\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "if USE_RANDOM_MUS:\n",
    "    test_settings[ \"ground_truth_mus\"] = [1,] # (np.random.uniform(0.1, 3, test_settings[\"num_of_sets\"])).tolist()\n",
    "    \n",
    "    random_settings_file = os.path.join(output_dir, \"random_mu.json\")\n",
    "    with open(random_settings_file, \"w\") as f:\n",
    "        json.dump(test_settings, f)\n",
    "else:\n",
    "    test_settings_file = os.path.join(input_dir, \"test\", \"settings\", \"data.json\")\n",
    "    with open(test_settings_file) as f:\n",
    "        test_settings = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Add directories to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.append(program_dir)\n",
    "path.append(submission_dir)\n",
    "path.append(score_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Internal imports\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import *\n",
    "from systematics import systematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Import Submission Model\n",
    "***\n",
    "We import a class named `Model` from the submission file (`model.py`). This `Model` class has the following methods:\n",
    "- `init`: receives train set and systematics class as input\n",
    "- `fit`: can be used for training\n",
    "- `predict`: receives one test set and outputs a dictionary with the following keys\n",
    "    - `mu_hat` : predicted mu $\\hat{\\mu}$\n",
    "    - `delta_mu_hat`: $\\Delta{\\hat{\\mu}}$ bound for $\\mu$\n",
    "    - `p16`: 16th percentile\n",
    "    - `p84`: 84th percentile\n",
    "\n",
    "In this example code, the `Model` class implements an XGBoost model which is trained to predict both the TES and the class label. You can find the code in `HEP-Challenge/sample_code_submission/model.py`. You can modify it the way you want, keeping the required class structure and functions there. More instructions are given inside the `model.py` file. If running in Collab, click the folder icon in the left sidebar to open the file browser.\n",
    "\n",
    "### ⚠️ Note:\n",
    "In real setting i.e. the challenge itself, the submitted model is initialized once with train set and systematics class and the `predict` is called multiple times, each time with a different test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "from datasets import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data\n",
    "***\n",
    "\n",
    "### ⚠️ Note:\n",
    "The data used here is a small sample data is for demonstration only to get a view of what the data looks like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`USE_PUBLIC_DATASET` determines whether to use a public dataset provided for the participants or use a small sample datafor quick execution of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PUBLIC_DATASET = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`USE_PUBLIC_DATASET` determines whether to use a public dataset provided for the participants or use a small subset of the data for quick execution of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PUBLIC_DATASET:\n",
    "    from datasets import Neurips2024_public_dataset as public_dataset\n",
    "    data = public_dataset()\n",
    "else:\n",
    "    data = Data(input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function loads the downloaded data in the public_data folder or downloads the data from codabench using `wget` in the absence of the downloaded data, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train set\n",
    "data.load_train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test sets\n",
    "data.load_test_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Visualize\n",
    "***\n",
    "- Visualize Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_visualize = Dataset_visualise(\n",
    "    data_set=data.get_train_set(),\n",
    "    columns=[\n",
    "        \"PRI_jet_leading_pt\",\n",
    "        \"PRI_met\",\n",
    "        \"DER_mass_vis\",\n",
    "        \"DER_mass_jet_jet\",\n",
    "        \"DER_sum_pt\",\n",
    "    ],\n",
    "    name=\"Train Set\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data summary\n",
    "train_visualize.examine_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data summary\n",
    "train_visualize.histogram_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_visualize.stacked_histogram(\"DER_mass_vis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data summary\n",
    "train_visualize.pair_plots(sample_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syst_train_data = data.get_syst_train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plots of train set with systematics\n",
    "train_visualize.pair_plots_syst(syst_train_data[\"data\"], sample_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstraped_data = data.generate_pseudo_exp_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data summary\n",
    "train_visualize.examine_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sum of weights = \", (bootstraped_data[\"weights\"]).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Program\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ingestion import Ingestion\n",
    "\n",
    "ingestion = Ingestion(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize submission\n",
    "ingestion.init_submission(Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Tools\n",
    "In the process of data fitting using our ingestion module, there are two primary arguments that control how the fit is performed: stat_only and syst_settings.\n",
    "- `stat_only (bool, optional)`: This argument, when set to True, forces the fitting process to compute statistics-only results. This means that only statistical uncertainties are considered, and all systematic uncertainties are ignored.\n",
    "- `syst_settings (dict, optional)`: This argument is a dictionary that specifies the systematic settings, indicating whether to fix certain systematics during the fitting process. Each entry in the dictionary represents a different systematic uncertainty, with a boolean value indicating whether it should be **fixed** (True) or allowed to **float** (False).\n",
    "\n",
    "Note that the `syst_settings` is used to only control the systematics that are allowed to float during the fitting process. It is different from the `TEST_SETTINGS['systematics']` which is used to specify the systematics to be used in the test set generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit submission\n",
    "ingestion.fit_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion.model.stat_analysis.nominal_histograms(1, apply_syst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load test set\n",
    "data.load_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predict submission\n",
    "ingestion.predict_submission(test_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion.compute_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save result\n",
    "ingestion.save_result(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Likelihood plots\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from systematics import postprocess, systematics\n",
    "from pathlib import Path\n",
    "# from systematics import LHC_NUMBERS as LHC_NUMBERS_NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL(mu, observed, expected_sig, expected_bkgd, scale=1, epsilon=1e-30):\n",
    "    # need to double check the scaling\n",
    "    expected = scale * (mu * expected_sig + expected_bkgd)\n",
    "    return np.sum(expected - observed * np.log(expected + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLLarray(mus, observed, expected_sig, expected_bkgd, epsilon=1e-30):\n",
    "    expected = (mus[:, None] * expected_sig + expected_bkgd)\n",
    "    return (expected - observed * np.log(expected + epsilon)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(mus, nlls):\n",
    "    mu_hat = mus[np.argmin(nlls)]\n",
    "    one_sigma = mus[(nlls - np.min(nlls)) < 0.5]\n",
    "    return mu_hat, (one_sigma[0], one_sigma[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sum of weights after post-cuts\n",
    "def check_weights_post_syst(data):\n",
    "    syst_data = systematics(data)\n",
    "    counts = {\n",
    "        name: syst_data['weights'][syst_data['detailed_labels'] == name].sum() for name in LHC_NUMBERS\n",
    "    }\n",
    "    return pd.Series(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_test_as_train(data_set=None):\n",
    "    joined_data = []\n",
    "    if data_set is None:\n",
    "        data.load_test_set()\n",
    "        data_set = data.get_test_set()\n",
    "    for name, df in data_set.items():\n",
    "        label = 1. if name == 'htautau' else 0.\n",
    "        joined_data.append(\n",
    "            df.assign(labels=label, detailed_labels=name)\n",
    "        )\n",
    "    joined_data = pd.concat(joined_data, axis=0, ignore_index=True, copy=False)\n",
    "    separated_data = {\n",
    "        'data': joined_data.drop(columns=['labels', 'detailed_labels', 'weights']),\n",
    "        'labels': joined_data['labels'],\n",
    "        'weights': joined_data['weights'],\n",
    "        'detailed_labels': joined_data['detailed_labels'],\n",
    "    }\n",
    "    return separated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 128\n",
    "hist_bins = np.linspace(0, 1, num_bins+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mus = np.linspace(0, 4, 10**4) # negtive mu breaks when using labels in place of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LHC_NUMBERS = {\n",
    "    \"ztautau\": 3544019,\n",
    "    \"diboson\": 40590,\n",
    "    \"ttbar\": 158761,\n",
    "    \"htautau\": 3639,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion.model.stat_analysis.holdout_scores = None\n",
    "tmp_results = ingestion.model.stat_analysis.vary_hist_bins(\n",
    "    bins_iter=tqdm(range(1, 11)),\n",
    "    data_set=format_test_as_train(data.get_test_set()),\n",
    "    mu_true=1,\n",
    "    plot_title='Without post-cuts',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_results_syst = ingestion.model.stat_analysis.vary_hist_bins(\n",
    "    bins_iter=tqdm(range(1, 11)),\n",
    "    data_set=format_test_as_train(data.get_test_set()),\n",
    "    apply_syst=True,\n",
    "    mu_true=1,\n",
    "    plot_title='With post-cuts',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale intervals to estimate the uncertainty with weighted events\n",
    "tmp_width_scale = np.sqrt(\n",
    "    sum(\n",
    "        (df['weights']**2).sum() for df in data.get_test_set().values()\n",
    "    ) / sum(\n",
    "        df['weights'].sum() for df in data.get_test_set().values()\n",
    "    )\n",
    ")\n",
    "tmp_inv_var_weighted_mean, tmp_weighted_var_inv = np.average(\n",
    "    [result['prediction']['mu_hat'] for result in tmp_results],\n",
    "    weights=[1 / (tmp_width_scale * result['prediction']['delta_mu_hat'] / 2)**2 for result in tmp_results],\n",
    "    returned=True,\n",
    ")\n",
    "tmp_weighted_var = 1 / tmp_weighted_var_inv\n",
    "tmp_mean_syst, tmp_var_inv_syst = np.average(\n",
    "    [result['prediction']['mu_hat'] for result in tmp_results_syst],\n",
    "    weights=[1 / (tmp_width_scale * result['prediction']['delta_mu_hat'] / 2)**2 for result in tmp_results_syst],\n",
    "    returned=True,\n",
    ")\n",
    "tmp_var_syst = 1 / tmp_var_inv_syst\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.errorbar(\n",
    "    [result['prediction']['mu_hat'] for result in tmp_results],\n",
    "    [result['bins'] for result in tmp_results],\n",
    "    xerr=[tmp_width_scale * result['prediction']['delta_mu_hat'] / 2 for result in tmp_results],\n",
    "    fmt='.',\n",
    "    label='without cuts',\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.errorbar(\n",
    "    [result['prediction']['mu_hat'] for result in tmp_results_syst],\n",
    "    [result['bins'] for result in tmp_results_syst],\n",
    "    xerr=[tmp_width_scale * result['prediction']['delta_mu_hat'] / 2 for result in tmp_results_syst],\n",
    "    fmt='.',\n",
    "    label='with cuts',\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.axvline(\n",
    "    tmp_inv_var_weighted_mean,\n",
    "    # color='red',\n",
    "    label='mean $\\hat \\mu$ without cuts',\n",
    ")\n",
    "plt.axvspan(\n",
    "    tmp_inv_var_weighted_mean - np.sqrt(tmp_weighted_var),\n",
    "    tmp_inv_var_weighted_mean + np.sqrt(tmp_weighted_var),\n",
    "    # color='red',\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.axvline(\n",
    "    tmp_mean_syst,\n",
    "    color='C1',\n",
    "    label='mean $\\hat \\mu$ with cuts',\n",
    ")\n",
    "plt.axvspan(\n",
    "    tmp_mean_syst - np.sqrt(tmp_var_syst),\n",
    "    tmp_mean_syst + np.sqrt(tmp_var_syst),\n",
    "    color='C1',\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.axvline(1, color='black', label='True $\\mu$')\n",
    "plt.axvline(1.05, color='black', linestyle='--', label='1.05')\n",
    "# plt.axvline(np.mean([result['prediction']['mu_hat'] for result in tmp_results]), color='red', label='Mean $\\hat \\mu$')\n",
    "# plt.axvspan(\n",
    "#     np.mean([result['prediction']['mu_hat'] for result in tmp_results]) - np.std([result['prediction']['mu_hat'] for result in tmp_results]) / np.sqrt(len(tmp_results)),\n",
    "#     np.mean([result['prediction']['mu_hat'] for result in tmp_results]) + np.std([result['prediction']['mu_hat'] for result in tmp_results]) / np.sqrt(len(tmp_results)),\n",
    "#     color='red',\n",
    "#     alpha=0.2,\n",
    "#     label='$ 1 \\sigma $'\n",
    "# )\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.ylabel('Number of bins')\n",
    "# plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('Full test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_luminosity_scale = .1\n",
    "\n",
    "tmp_holdout_set = ingestion.model.stat_analysis.holdout_set.copy()\n",
    "ingestion.m['weights'] == tmp_luminosity_scale * tmp_holdout_set['weights'].copy()\n",
    "\n",
    "tmp_data_set = format_test_as_train(data.get_test_set()).copy()\n",
    "tmp_data_set['weights'] == tmp_luminosity_scale * tmp_data_set['weights'].copy()\n",
    "\n",
    "# ingestion.model.stat_analysis.holdout_set['weights'] *= tmp_luminosity_scale\n",
    "\n",
    "results_low_luminosity = ingestion.model.stat_analysis.vary_hist_bins(\n",
    "    # bins_iter=tqdm(range(1, 11)),\n",
    "    data_set=tmp_data_set,\n",
    "    mu_true=1,\n",
    "    plot_title='Without post-cuts',\n",
    ")\n",
    "results_low_luminosity_syst = ingestion.model.stat_analysis.vary_hist_bins(\n",
    "    # bins_iter=tqdm(range(1, 11)),\n",
    "    data_set=tmp_data_set,\n",
    "    mu_true=1,\n",
    "    apply_syst=True,\n",
    "    plot_title='With post-cuts',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data_set['weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking event counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.load_train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weighted_data = data.get_train_set().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in all_weighted_data.items():\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weighted_data['detailed_labels'] = np.array(all_weighted_data['detailed_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asimov_data = format_test_as_train(data.get_test_set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {name: all_weighted_data['weights'][all_weighted_data['detailed_labels'] == name] for name in LHC_NUMBERS}\n",
    "# plt.figure(figsize=(16, 10))\n",
    "plt.subplots(2, 2, figsize=(16, 10))\n",
    "for j, name in enumerate(LHC_NUMBERS, start=1):\n",
    "    plt.subplot(2, 2, j)\n",
    "    tmp_weights = all_weighted_data['weights'][all_weighted_data['detailed_labels'] == name]\n",
    "    print(name, tmp_weights.shape, tmp_weights.mean(), tmp_weights.std())\n",
    "    print(pd.Series(tmp_weights).value_counts())\n",
    "    plt.hist(tmp_weights, bins=100, density=False, alpha=0.5, label=name)\n",
    "    plt.title(name)\n",
    "# plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('weights')\n",
    "    plt.ylabel('count')\n",
    "    plt.legend()\n",
    "# plt.title('weighted data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_weighted_data['weights'][all_weighted_data['detailed_labels'] == 'htautau'] == 0).sum(), (all_weighted_data['detailed_labels'] == 'htautau').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        name: {\n",
    "            # 'LHC_NUMBERS (old)': LHC_NUMBERS[name],\n",
    "            'template sum of weights': all_weighted_data['weights'][all_weighted_data['detailed_labels'] == name].sum(),\n",
    "            'asimov sum of weights': asimov_data['weights'][asimov_data['detailed_labels'] == name].sum(),\n",
    "            'template num events': (all_weighted_data['detailed_labels'] == name).sum(),\n",
    "            'asimov num events': (asimov_data['detailed_labels'] == name).sum(),\n",
    "            'template average weight': all_weighted_data['weights'][all_weighted_data['detailed_labels'] == name].mean(),\n",
    "            'asimov average weight': asimov_data['weights'][asimov_data['detailed_labels'] == name].mean(),\n",
    "        }\n",
    "        for name in LHC_NUMBERS\n",
    "    },\n",
    "    orient='index',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df.T.style.format(precision=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df['template sum post cuts'] = check_weights_post_syst(data.get_train_set())\n",
    "counts_df['asimov sum post cuts'] = check_weights_post_syst(format_test_as_train(data.get_test_set()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df.assign(diff=lambda df: df['asimov sum post cuts'] - df['template sum post cuts']).T.assign(sum=lambda df: df.sum(1)).style.format(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "Use scores and weights from Sascha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bresult = [] # list of dictionaries of results of varying num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sresults = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sascha_dir = Path('/global/ml4hep/spss/jvdudley/physicsData/FairUniverse/comparison_Sascha/Fair_Universe_Comparison_Data/')\n",
    "sascha_dir = Path('/global/ml4hep/spss/sdiefenbacher/Fair_Universe/comparison/comparison_templates_data_07_23_seed45/')\n",
    "# sprefix = 'Run_9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in sascha_dir.glob('eval*.npy'):\n",
    "    # if file.stem.startswith('Run'): continue\n",
    "    print(file.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sread(pattern, dir=sascha_dir):\n",
    "    return {file.stem: np.load(file) for file in dir.glob(pattern)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sread('eval*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sread('Run_0*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pattern in ('eval*.npy', 'Run_0*.npy', '*bkg*.npy'):\n",
    "    plt.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdata = {}\n",
    "sedata = {}\n",
    "\n",
    "sdata = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pattern in ('*sig*.npy', '*bkg*.npy', '*Run_*.npy'):\n",
    "for name, data in sread('*.npy').items():\n",
    "    # print(name, data.shape)\n",
    "    if name.endswith('_weights'):\n",
    "        tmpsuffix = 'weights'\n",
    "    elif name.endswith('_values'):\n",
    "        tmpsuffix = 'values'\n",
    "    else:\n",
    "        raise ValueError(f'{name} does not end with \"_weights\" or \"_values\"')\n",
    "    tmpprefix = name.removesuffix('_weights').removesuffix('_values')\n",
    "    if tmpprefix.startswith('Run_'):\n",
    "        tmpprefix = int(tmpprefix.removeprefix('Run_').removesuffix('eval'))\n",
    "    if tmpprefix not in sdata:\n",
    "        sdata[tmpprefix] = {}\n",
    "    sdata[tmpprefix][tmpsuffix] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in sread('*Run_*.npy').items():\n",
    "    tmpprefix = name.removesuffix('_weights').removesuffix('_values')\n",
    "    if tmpprefix not in sedata:\n",
    "        sedata[tmpprefix] = {}\n",
    "    sedata[tmpprefix][name.removeprefix(tmpprefix + '_')] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sedata['Run_0eval'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sascha_copy = {file.stem: np.load(file) for file in sascha_dir.glob('*bkg*.npy')}\n",
    "sascha_data = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in sascha_copy:\n",
    "    tmp_prefix = name.removesuffix('_values').removesuffix('_weights')\n",
    "    if tmp_prefix not in sascha_data:\n",
    "        sascha_data[tmp_prefix] = {}\n",
    "    sascha_data[tmp_prefix][name.removeprefix(tmp_prefix + '_')] = sascha_copy[name]\n",
    "del sascha_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sascha_hist = {name: np.histogram(data['values'], bins=hist_bins, weights=data['weights'])[0] for name, data in sascha_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shist = {name: np.histogram(data['values'], bins=hist_bins, weights=data['weights'])[0] for name, data in tqdm(sdata.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svar = {name: np.histogram(data['values'], bins=hist_bins, weights=data['weights']**2)[0] for name, data in tqdm(sdata.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sthist = {name: np.histogram(data['values'], bins=hist_bins, weights=data['weights'])[0] for name, data in stdata.items()}\n",
    "sehist = {name: np.histogram(data['values'], bins=hist_bins, weights=data['weights'])[0] for name, data in sedata.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.sqrt(shist['template_sig'] + shist['template_bkg']) / np.sqrt((svar['template_sig'] + svar['template_bkg']))).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stairs(\n",
    "    np.sqrt(shist['template_sig'] + shist['template_bkg']),\n",
    "    hist_bins,\n",
    "    label='Poisson uncertainty',\n",
    "    # color='blue',\n",
    "    alpha=0.5,\n",
    "    fill=True,\n",
    ")\n",
    "plt.stairs(\n",
    "    np.sqrt(svar['template_sig'] + svar['template_bkg']),\n",
    "    hist_bins,\n",
    "    label='MC uncertainty',\n",
    "    # color='red',\n",
    "    alpha=0.5,\n",
    "    fill=True,\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.axvline(\n",
    "    hist_bins[shist['template_sig'].argmax()] + (hist_bins[1] - hist_bins[0]) / 2,\n",
    "    color='red',\n",
    "    linestyle='--',\n",
    "    label='max sig',\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.axvline(\n",
    "    hist_bins[(shist['template_sig'] / shist['template_bkg']).argmax()] + (hist_bins[1] - hist_bins[0]) / 2,\n",
    "    color='red',\n",
    "    linestyle=':',\n",
    "    label='max sig/bkg',\n",
    "    alpha=0.5,\n",
    ")\n",
    "# plt.hist(\n",
    "#     [hist_bins[:-1],] * 2,\n",
    "#     hist_bins,\n",
    "#     weights=[hist for name, hist in shist.items() if isinstance(name, str)],\n",
    "#     alpha=0.5,\n",
    "#     label=[name for name in shist if isinstance(name, str)],\n",
    "#     histtype='barstacked',\n",
    "# )\n",
    "# tmp_vals = 0\n",
    "# tmp_baseline = 0\n",
    "# for sb in ('template_bkg', 'template_sig'):\n",
    "#     tmp_vals += shist[sb]\n",
    "#     plt.stairs(\n",
    "#         tmp_vals,\n",
    "#         hist_bins,\n",
    "#         alpha=0.5,\n",
    "#         label=sb,\n",
    "#         baseline=tmp_baseline,\n",
    "#         fill=True,\n",
    "#     )\n",
    "#     tmp_baseline += shist[sb]\n",
    "plt.stairs(\n",
    "    shist['template_sig'],\n",
    "    hist_bins,\n",
    "    alpha=0.5,\n",
    "    label='sig',\n",
    "    fill=True,\n",
    ")\n",
    "plt.errorbar(\n",
    "    (hist_bins[:-1] + hist_bins[1:]) / 2,\n",
    "    shist['template_sig'], # + shist['template_bkg']\n",
    "    1 * np.sqrt(svar['template_sig'] + svar['template_bkg']),\n",
    "    fmt=',',\n",
    "    color='black',\n",
    "    # drawstyle='steps-mid',\n",
    "    label='bkg+sig uncertainty',\n",
    "    alpha=0.5,\n",
    ")\n",
    "for name, hist in shist.items():\n",
    "    if isinstance(name, str):\n",
    "        continue\n",
    "    plt.stairs( # hist(\n",
    "        hist - shist['template_bkg'], # hist_bins[:-1],\n",
    "        hist_bins,\n",
    "        # weights=hist - shist['template_bkg'],\n",
    "        alpha=0.01,\n",
    "        # label=name,\n",
    "        # histtype='step',\n",
    "    )\n",
    "# plt.yscale('log')\n",
    "# plt.xlim(.9, 1)\n",
    "# plt.ylim(4e1, 1.3e3)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title('pseudoexperiments - bkg template')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sascha_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sascha_nlls = [\n",
    "#     NLL(\n",
    "#         mu,\n",
    "#         sascha_hist[sprefix + 'eval'],\n",
    "#         sascha_hist[sprefix + 'template_sig'],\n",
    "#         sascha_hist[sprefix + 'template_bkg'],\n",
    "#     ) for mu in mus\n",
    "# ]\n",
    "sascha_nlls = NLLarray(\n",
    "    mus,\n",
    "    shist[0],\n",
    "    shist['template_sig'],\n",
    "    shist['template_bkg'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sascha_one_sigma = np.array(mus)[(sascha_nlls - min(sascha_nlls)) < 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sresults[sprefix] = (sascha_one_sigma[0], sascha_one_sigma[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mus, sascha_nlls - min(sascha_nlls))\n",
    "plt.axhline(.5, color='black', linestyle='--')\n",
    "plt.axvline(mus[np.argmin(sascha_nlls)], color='C1', label=f'mu_hat: {mus[np.argmin(sascha_nlls)]:.2f}')\n",
    "# plt.axvline(sascha_one_sigma[0], color='r', label=f'p16: {sascha_one_sigma[0]:.2f}')\n",
    "# plt.axvline(sascha_one_sigma[-1], color='r', label=f'p84: {sascha_one_sigma[-1]:.2f}')\n",
    "plt.axvspan(sascha_one_sigma[0], sascha_one_sigma[-1], color='grey', alpha=0.5, label=f'p16-p84: {sascha_one_sigma[0]:.2f}-{sascha_one_sigma[-1]:.2f}')\n",
    "# plt.axvline(1.45, color='C2', label='mu_hat (Sascha)')\n",
    "# plt.axvline(1.3952, color='C3', label='mu_hat (Yulei)')\n",
    "plt.axvline(1, color='black', label='mu_true')\n",
    "# plt.plot(mus, 5.1 * (mus - mus[np.argmin(sascha_nlls)])**2, label='(mu - mu_hat)^2')\n",
    "plt.ylim(0, 2)\n",
    "plt.xlabel('mu')\n",
    "plt.ylabel('NLL - min(NLL)')\n",
    "plt.legend()\n",
    "plt.title('Comparison:' + sprefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sresults = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, hist in tqdm(shist.items(), 'computing results from histograms'):\n",
    "    # print(name)\n",
    "    if isinstance(name, str):\n",
    "        continue\n",
    "    snlls = NLLarray(mus, hist, shist['template_sig'], shist['template_bkg'])\n",
    "    #     NLL(\n",
    "    #         mu,\n",
    "    #         hist,\n",
    "    #         shist['template_sig'],\n",
    "    #         shist['template_bkg'],\n",
    "    #     ) for mu in mus\n",
    "    # ]\n",
    "    sresults[name] = get_results(mus, snlls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = np.array([t for _, t in sresults.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert mus[1] < intervals.min(), f'mus starts too high'\n",
    "print(f'{mus[1]:.3f} < {intervals.min():.3f}')\n",
    "assert mus[-2] > intervals.max(), f'mus ends too low'\n",
    "print(f'{mus[-2]:.3f} > {intervals.max():.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bresult[num_bins] = (intervals.mean(), ((intervals[:, 0] < 1) & (intervals[:, 1] > 1)).mean())\n",
    "bresult.append({\n",
    "    'mean': intervals.mean(),\n",
    "    'stderr': intervals.mean(1).std() / np.sqrt(intervals.shape[0]),\n",
    "    'coverage': ((intervals[:, 0] < 1) & (intervals[:, 1] > 1)).mean(),\n",
    "    'num_pseudo_experiments': intervals.shape[0],\n",
    "    'num_bins': num_bins,\n",
    "})\n",
    "\n",
    "# print(f'mean: {bresult[num_bins][0]:.3f}, coverage: {bresult[num_bins][1]:.3f}')\n",
    "print(bresult[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.hlines(\n",
    "    [name for name in sresults],\n",
    "    [p16 for _, (p16, _) in sresults.values()],\n",
    "    [p84 for _, (_, p84) in sresults.values()],\n",
    ")\n",
    "# plot mu_true: currently 1\n",
    "plt.axvline(1, color='black', label=f'mu_true=1')\n",
    "# plot mu_hat mean\n",
    "plt.axvline(bresult[-1]['mean'], color='r', label=f'mu_hat mean={bresult[-1][\"mean\"]:.3f}')\n",
    "# plot mu_hat stderr\n",
    "plt.axvspan(\n",
    "    bresult[-1]['mean'] - bresult[-1]['stderr'],\n",
    "    bresult[-1]['mean'] + bresult[-1]['stderr'],\n",
    "    color='r',\n",
    "    alpha=0.25,\n",
    "    label=f'mu_hat stderr={bresult[-1][\"stderr\"]:.3f}',\n",
    ")\n",
    "plt.xlabel('mu')\n",
    "plt.ylabel('pseudo-experiment')\n",
    "plt.title(f'{num_bins} histogram bins: coverage={bresult[-1][\"coverage\"]:.3f}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bresult plots of mean±stderr and coverage vs num_bins\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "plt.errorbar(\n",
    "    [b['mean'] for b in bresult],\n",
    "    [b['num_bins'] for b in bresult],\n",
    "    xerr=[b['stderr'] for b in bresult],\n",
    "    fmt='o',\n",
    "    label='mu_hat',\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.axvline(1, color='black', label='mu_true')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('num_bins')\n",
    "plt.xlabel('mu_hat')\n",
    "plt.twiny()\n",
    "plt.errorbar(\n",
    "    [b['coverage'] for b in bresult],\n",
    "    [b['num_bins'] for b in bresult],\n",
    "    xerr=[np.sqrt(.682689 * (1 - .682689) / b['num_pseudo_experiments']) for b in bresult],\n",
    "    fmt='ro',\n",
    "    label='coverage',\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.axvline(.682689, color='r', linestyle='--', label='1 sigma')\n",
    "# plt.axhspan(\n",
    "#     .682689 - np.sqrt(.682689 * (1 - .682689) / bresult[-1]['num_pseudo_experiments']),\n",
    "#     .682689 + np.sqrt(.682689 * (1 - .682689) / bresult[-1]['num_pseudo_experiments']),\n",
    "#     color='grey',\n",
    "#     alpha=0.25,\n",
    "# )\n",
    "plt.xlabel('coverage')\n",
    "plt.suptitle('Results of varying num_bins')\n",
    "plt.text(.557, .9, sascha_dir.stem)\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cresults = {\n",
    "    'Jordan': {\n",
    "        'Run_0': (0.8823613236132362, 1.1882118821188212),\n",
    "        'Run_1': (0.8945064450644507, 1.1997444974449745),\n",
    "        'Run_2': (0.8033480334803348, 1.115131151311513),\n",
    "        'Run_3': (0.9987899878998789, 1.3043430434304342),\n",
    "        'Run_4': (0.9687946879468795, 1.2695876958769587),\n",
    "        'Run_5': (0.9235392353923539, 1.232959829598296),\n",
    "        'Run_6': (1.180021800218002, 1.4898273982739827),\n",
    "        'Run_7': (1.091540915409154, 1.4003315033150332),\n",
    "        'Run_8': (1.0722382223822238, 1.378928789287893),\n",
    "        'Run_9': (1.0410879108791087, 1.3475684756847568),\n",
    "    },\n",
    "    'Sascha': {\n",
    "        'Run_0': (0.901194159732266, 1.2062761636214896),\n",
    "        'Run_1': (0.8894361103293542, 1.1950205736920794),\n",
    "        'Run_2': (0.8156946813859971, 1.1266712540266663),\n",
    "        'Run_3': (0.9982769846663299, 1.3037225824265635),\n",
    "        'Run_4': (0.9674725796855451, 1.2681422564378708),\n",
    "        'Run_5': (0.9341876695842286, 1.2431236399605807),\n",
    "        'Run_6': (1.210834505642529, 1.5191684157141783),\n",
    "        'Run_7': (1.1024395216920113, 1.4106339865777082),\n",
    "        'Run_8': (1.0826025589257644, 1.388693180595792),\n",
    "        'Run_9': (1.0624547927260337, 1.3678220195383683),\n",
    "    },\n",
    "    'Yulei': [\n",
    "        (0.8641240477713032, 1.1685347528269672),\n",
    "        (0.8534984694263906, 1.1566380803920224),\n",
    "        (0.8163559424937542, 1.1282676529144529),\n",
    "        (1.0138248845573359, 1.3189941669186172),\n",
    "        (0.9976235681984136, 1.2983202394845823),\n",
    "        (0.8384660075098151, 1.145517502230531),\n",
    "        (1.0601399318867162, 1.3669774991943608),\n",
    "        (1.0648479946635852, 1.3725732313530639),\n",
    "        (1.1072575393906805, 1.4142756561289402),\n",
    "        (0.9287805137600891, 1.2318307738852956),\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cresults['Yulei'] = {f'Run_{j}': vals for j, vals in enumerate(cresults['Yulei'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.axvline(1, color='black', label='mu_true=1')\n",
    "for j, (name, results) in enumerate(cresults.items()):\n",
    "    plt.hlines(\n",
    "        [int(run.removeprefix('Run_')) + j/10 for run in results],\n",
    "        [p16 for p16, _ in results.values()],\n",
    "        [p84 for _, p84 in results.values()],\n",
    "        color=f'C{j}',\n",
    "        label=name,\n",
    "    )\n",
    "plt.xlabel('mu')\n",
    "plt.ylabel('Run_[number]')\n",
    "plt.legend()\n",
    "plt.title('Coverage comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict mu for test data\n",
    "\n",
    "This does not include postselection cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data: train, valid, test\n",
    "train_set = ingestion.model.training_set.copy()\n",
    "valid_set = ingestion.model.stat_analysis.holdout_set.copy()\n",
    "ingestion.data.load_test_set()\n",
    "test_set = ingestion.data.get_test_set().copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_exp_data = ingestion.data.generate_pseudo_exp_data(seed=3141592653)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (pseudo_exp_data['weights'] == 1).all(), 'pseudo experiment data weights are not all one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df['num unweighted events'] = {name: df.shape[0] for name, df in data.get_test_set().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df.T.style.format(precision=3, thousands=',').format(precision=6, subset=('average weight',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums_by_type = {\n",
    "    name: (\n",
    "        # all_weighted_data['weights'][all_weighted_data['detailed_labels'] == name].sum(),\n",
    "        train_set['weights'][train_set['detailed_labels'] == name].sum(),\n",
    "        valid_set['weights'][valid_set['detailed_labels'] == name].sum(),\n",
    "    )\n",
    "    for name in LHC_NUMBERS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums_by_type['bkgd'] = (\n",
    "    all_weighted_data['weights'][all_weighted_data['labels'] == 0].sum(),\n",
    "    train_set['weights'][train_set['labels'] == 0].sum(),\n",
    "    valid_set['weights'][valid_set['labels'] == 0].sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, sums in sums_by_type.items():\n",
    "    print(f'{name}:\\nall: {sums[0]:.3f}, train: {sums[1]:.3f}, valid: {sums[2]:.3f}')\n",
    "    print(f'min: {min(sums[1:]):.3f}\\nmax: {max(sums[1:]):.3f}')\n",
    "    # print(min(sums[1:]) < sums[0] < max(sums[1:]))\n",
    "    if sums[0] < min(sums[1:]):\n",
    "        print(f'Warning: {name} sum of weights in all data less than in train and valid data')\n",
    "    elif sums[0] > max(sums[1:]):\n",
    "        print(f'Warning: {name} sum of weights in all data greater than in train and valid data')\n",
    "    else:\n",
    "        print(f'{name} sum of weights in all data is between train and valid data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace LHC_NUMBERS[name] with counts_df['sum of weights'][name].round().astype(int)\n",
    "\n",
    "# for key in test_set:\n",
    "#     print(key, test_set[key].shape)\n",
    "#     assert test_set[key].shape[0] >= LHC_NUMBERS[key], f'{key} test set is too small'\n",
    "#     test_set[key] = test_set[key].copy()[:LHC_NUMBERS[key]]\n",
    "print(data.get_test_set()['htautau'].shape)\n",
    "test_set_samples = {}\n",
    "for name, df in test_set.items():\n",
    "    print(f'{name}: unweighted events={df.shape[0]}, old expectation={LHC_NUMBERS[name]}, ratio={df.shape[0] / LHC_NUMBERS[name]}, new expectation={LHC_NUMBERS_NEW[name]}')\n",
    "    assert df.shape[0] > LHC_NUMBERS_NEW[name], f'{name} test set is too small'\n",
    "    test_set_samples[name] = df[:int(LHC_NUMBERS_NEW[name] + .5)]\n",
    "test_set = test_set_samples\n",
    "print(data.get_test_set()['htautau'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_syst = systematics(valid_set.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_syst['weights'].sum(), pseudo_exp_data['weights'].sum(), sum((df.shape[0] for df in test_set.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = valid_syst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in test_set:\n",
    "    print(name, '\\n', test_set[name].shape)\n",
    "    test_set[name] = postprocess(test_set[name])\n",
    "    print(test_set[name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_expected = valid_set['weights'].sum()\n",
    "n_observed = np.sum([data.shape[0] for data in test_set.values()])\n",
    "n_pseudo = pseudo_exp_data['data'].shape[0]\n",
    "\n",
    "n_expected, n_observed, n_pseudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['scores'] = ingestion.model.model.predict(train_set['data'])\n",
    "valid_set['scores'] = ingestion.model.model.predict(valid_set['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pseudo_exp_data['scores'] = ingestion.model.model.predict(pseudo_exp_data['data'], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = {\n",
    "    name: ingestion.model.model.predict(test_set[name].drop('weights', axis=1))\n",
    "    for name in test_set.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_to_result(num_bins, observed, expected=valid_set):\n",
    "    hist_bins = np.linspace(0, 1, num_bins + 1)\n",
    "    sig_hist, _ = np.histogram(expected['scores'][expected['labels'] == 1], hist_bins, weights=expected['weights'][expected['labels'] == 1])\n",
    "    bkg_hist, _ = np.histogram(expected['scores'][expected['labels'] == 0], hist_bins, weights=expected['weights'][expected['labels'] == 0])\n",
    "    obs_hist, _ = np.histogram(observed['scores'], hist_bins, weights=observed['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms = { # train/valid, sig/bkgd\n",
    "    tv + sb: np.histogram(\n",
    "        dataset['scores'][dataset['labels'] == (1-j)], # testing using the label as the score\n",
    "        bins=hist_bins,\n",
    "        weights=dataset['weights'][dataset['labels'] == (1-j)],\n",
    "        # range=(0, 1),\n",
    "    )\n",
    "    for tv, dataset in {'train': train_set, 'valid': valid_set}.items()\n",
    "    for j, sb in enumerate(['_sig', '_bkgd'])\n",
    "}\n",
    "pseudo_exp_hist = np.histogram(\n",
    "    pseudo_exp_data['scores'], # testing using the label as the score\n",
    "    bins=hist_bins,\n",
    "    # weights=pseudo_exp_data['weights'], # weights are all 1\n",
    "    # range=(0, 1),\n",
    ")\n",
    "test_hist = {\n",
    "    name: np.histogram(\n",
    "        scores,\n",
    "        bins=hist_bins,\n",
    "        weights=test_set[name]['weights'],\n",
    "        # range=(0, 1),\n",
    "    )\n",
    "    for name, scores in test_scores.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(histograms['valid_bkgd'][0] + histograms['valid_sig'][0]).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms_var ={\n",
    "    'valid_sig': np.histogram(\n",
    "        valid_set['scores'][valid_set['labels'] == 1],\n",
    "        bins=hist_bins,\n",
    "        weights=valid_set['weights'][valid_set['labels'] == 1]**2,\n",
    "    )[0],\n",
    "    'valid_bkgd':np.histogram(\n",
    "        valid_set['scores'][valid_set['labels'] == 0],\n",
    "        bins=hist_bins,\n",
    "        weights=valid_set['weights'][valid_set['labels'] == 0]**2,\n",
    "    )[0],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(histograms['valid_bkgd'][0] + histograms['valid_sig'][0] - 1 * np.sqrt(histograms_var['valid_bkgd'] + histograms_var['valid_sig'])).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(histograms['valid_bkgd'][0] - 1 * np.sqrt(histograms_var['valid_bkgd'])).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_exp_hist[0].min(), np.sum([hist for hist, _ in test_hist.values()], axis=0).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pseudo_exp_hist[0] == 0).sum(), pseudo_exp_hist[0].argmin(), pseudo_exp_hist[0][:5], pseudo_exp_hist[0][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density = False\n",
    "plot_scaled = False\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "for name, (hist, bins) in histograms.items():\n",
    "    plt.hist(\n",
    "        bins[:-1],\n",
    "        bins,\n",
    "        weights=hist * ((n_observed / n_expected) if plot_scaled else 1),\n",
    "        label=name + (' (scaled)' if plot_scaled else ''),\n",
    "        density=plot_density,\n",
    "        histtype='step', # if 'train' in name else 'bar',\n",
    "        alpha=.5 if 'valid' in name else 0.2,\n",
    "    )\n",
    "plt.hist(\n",
    "    hist_bins[:-1],\n",
    "    hist_bins,\n",
    "    weights=pseudo_exp_hist[0],\n",
    "    label='pseudo-experiment',\n",
    "    density=plot_density,\n",
    "    histtype='step',\n",
    "    alpha=.5,\n",
    ")\n",
    "plt.stairs(\n",
    "    histograms['valid_bkgd'][0] + histograms['valid_sig'][0],\n",
    "    hist_bins,\n",
    "    label='valid total',\n",
    ")\n",
    "# for name, (hist, bins) in test_hist.items():\n",
    "#     plt.hist(\n",
    "#         bins[:-1],\n",
    "#         bins,\n",
    "#         weights=hist,\n",
    "#         label=(name + ' (test set)'),\n",
    "#         density=plot_density,\n",
    "#         histtype='bar',\n",
    "#         stacked=True,\n",
    "#         alpha=.5,\n",
    "#     )\n",
    "# tmp_hists = []\n",
    "# tmp_weights = []\n",
    "plt.hist(\n",
    "    # [bins[:-1] for (hist, bins) in test_hist.values()],\n",
    "    [hist_bins[:-1]] * len(test_hist),\n",
    "    # test_hist['htautau'][1], # all bins are the same; pick one\n",
    "    hist_bins,\n",
    "    weights=[hist for (hist, bins) in test_hist.values()],\n",
    "    label=[name for name in test_hist],\n",
    "    density=plot_density,\n",
    "    histtype='barstacked',\n",
    ")\n",
    "# plt.ylim(0, 11e3)\n",
    "# plt.ylim(1e3, 3e5)\n",
    "# plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlls = [\n",
    "    NLL(\n",
    "        mu,\n",
    "        histograms['train_sig'][0] + histograms['train_bkgd'][0],\n",
    "        histograms['valid_sig'][0],\n",
    "        histograms['valid_bkgd'][0],\n",
    "    ) for mu in mus\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mus, nlls - min(nlls))\n",
    "plt.axhline(.5, color='r', linestyle='--')\n",
    "plt.axvline(1, color='g', label='mu_true')\n",
    "plt.axvline(mus[np.argmin(nlls)], color='r', label='mu_hat')\n",
    "plt.xlabel('mu')\n",
    "plt.ylabel('NLL')\n",
    "plt.ylim(0, 2.5)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title('training set')\n",
    "\n",
    "f'mu_hat = {mus[np.argmin(nlls)]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nlls = [\n",
    "    NLL(\n",
    "        mu,\n",
    "        # test_hist['htautau'][0] + test_hist['ztautau'][0],\n",
    "        np.sum([hist for (hist, bins) in test_hist.values()], axis=0),\n",
    "        histograms['valid_sig'][0],\n",
    "        histograms['valid_bkgd'][0],\n",
    "        # scale=(n_observed / n_expected),\n",
    "    ) for mu in mus\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mus, test_nlls - min(test_nlls))\n",
    "plt.axhline(.5, color='r', linestyle='--')\n",
    "plt.axvline(1, color='black', label='mu_true')\n",
    "plt.xlabel('mu')\n",
    "plt.ylabel('NLL')\n",
    "# plt.xlim(.75, 1.35)\n",
    "# plt.ylim(0, 2.5)\n",
    "plt.title('test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_nlls = [\n",
    "    NLL(\n",
    "        mu,\n",
    "        pseudo_exp_hist[0],\n",
    "        histograms['valid_sig'][0],\n",
    "        histograms['valid_bkgd'][0],\n",
    "        # scale=(n_observed / n_expected),\n",
    "    ) for mu in mus\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mus, pseudo_nlls - min(pseudo_nlls))\n",
    "plt.axhline(.5, color='r', linestyle='--')\n",
    "plt.axvline(1, color='black', label='mu_true')\n",
    "plt.xlabel('mu')\n",
    "plt.ylabel('NLL - min(NLL)')\n",
    "# plt.ylim(0, 2.5)\n",
    "plt.title('pseudo experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, v in pseudo_exp_data.items():\n",
    "    print(name, v.shape[0], name in pseudo_exp_data['data'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_exp_counts, pseudo_exp_data['detailed_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_init = 3141592653\n",
    "num_pseudo_experiments = 100\n",
    "pseudo_results = []\n",
    "mu_true = 1\n",
    "pseudo_exp_frac = .1\n",
    "\n",
    "# for seed in range(seed_init, seed_init + num_pseudo_experiments):\n",
    "for j in tqdm(range(num_pseudo_experiments), f'Running {num_pseudo_experiments} pseudo-experiments'):\n",
    "    # print(f'{j / num_pseudo_experiments:.2%} complete', end='\\r', flush=True)\n",
    "    seed = seed_init + j\n",
    "    pseudo_exp_data, pseudo_exp_counts = ingestion.data.generate_pseudo_exp_data(mu_true, seed=seed, return_counts=True, lhc_frac=pseudo_exp_frac)\n",
    "    # pseudo_exp_data = {name: v[:int(v.shape[0] / 10 + .5)] for name, v in pseudo_exp_data.items()} # only keep the first 10% of the data\n",
    "    # pseudo_exp_counts = pseudo_exp_data['detailed_labels'].value_counts().to_dict()\n",
    "    pseudo_exp_data['scores'] = ingestion.model.model.predict(pseudo_exp_data['data'], verbose=0)\n",
    "    pseudo_exp_hist = np.histogram(\n",
    "        pseudo_exp_data['scores'], # testing using the labels in place of the scores\n",
    "        bins=hist_bins,\n",
    "    )\n",
    "    # pseudo_nlls = NLLarray(mus, pseudo_exp_hist[0], histograms['valid_sig'][0], histograms['valid_bkgd'][0])\n",
    "    pseudo_nlls = [\n",
    "        NLL(\n",
    "            mu,\n",
    "            pseudo_exp_hist[0],\n",
    "            histograms['valid_sig'][0] * pseudo_exp_frac,\n",
    "            histograms['valid_bkgd'][0] * pseudo_exp_frac,\n",
    "        ) for mu in mus\n",
    "    ]\n",
    "    pseudo_results.append((seed, get_results(mus, pseudo_nlls), pseudo_exp_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = np.array([result[1][1] for result in pseudo_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_fractions(intervals, mu_true=1, max_sigma=5):\n",
    "    def n_sigma(n, means, intervals):\n",
    "        return n * (intervals - means[:, None]) + means[:, None]\n",
    "    means = intervals.mean(1)\n",
    "    num_samples = intervals.shape[0]\n",
    "    frac_within = [\n",
    "        ((n_sigma(j, means, intervals)[:, 0] < mu_true) & (mu_true < n_sigma(j, means, intervals)[:, 1])).sum() / num_samples\n",
    "        for j in range(1, max_sigma + 1)\n",
    "    ]\n",
    "    return frac_within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, frac in enumerate(sigma_fractions(intervals, mu_true)):\n",
    "    print(f'fraction within {j+1} sigma: {frac:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for *_, counts in pseudo_results:\n",
    "#     print(counts['htautau'] / mu_true)\n",
    "\n",
    "valid_set['weights'][valid_set['labels'] == 1].sum(), test_set['htautau'].shape[0], np.mean([counts['htautau'] for *_, counts in pseudo_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_stats['ddof'] = 1\n",
    "\n",
    "tmp_stats['mu_hat_mean'] = intervals.mean()\n",
    "tmp_stats['mu_hat_std'] = intervals.mean(1).std(ddof=tmp_stats['ddof'])\n",
    "tmp_stats['mu_hat_stderr'] = tmp_stats['mu_hat_std'] / np.sqrt(intervals.shape[0])\n",
    "tmp_stats['width_mean'] = (intervals[:, 1] - intervals[:, 0]).mean()\n",
    "tmp_stats['width_std'] = (intervals[:, 1] - intervals[:, 0]).std(ddof=tmp_stats['ddof'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_stats['mu_hat_std'] * 2, tmp_stats['width_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tmp_stats['mu_hat_mean'] - mu_true) / tmp_stats['mu_hat_stderr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "# plot mu_hat\n",
    "plt.plot(\n",
    "    [result[1][0] for result in pseudo_results],\n",
    "    [seed for seed, *_ in pseudo_results],\n",
    "    '.',\n",
    "    label='mu_hat',\n",
    "    # alpha=0.5,\n",
    ")\n",
    "# plot mu_hat error bars\n",
    "plt.hlines(\n",
    "    [seed for seed, *_ in pseudo_results],\n",
    "    [result[1][1][0] for result in pseudo_results],\n",
    "    [result[1][1][1] for result in pseudo_results],\n",
    "    # linestyles='solid',\n",
    "    # label='p16-p84',\n",
    "    # alpha=0.5,\n",
    ")\n",
    "# plot mu_true\n",
    "plt.axvline(mu_true, color='black', label=f'{mu_true=}')\n",
    "# plot mu_hat mean\n",
    "plt.axvline(intervals.mean(1).mean(), color='r', label=f'mu_hat_mean={intervals.mean(1).mean():.3f}')\n",
    "# plot mu_hat stderr\n",
    "plt.axvspan(\n",
    "    intervals.mean() - intervals.mean(1).std() / np.sqrt(intervals.shape[0]),\n",
    "    intervals.mean() + intervals.mean(1).std() / np.sqrt(intervals.shape[0]),\n",
    "    color='r',\n",
    "    alpha=0.25,\n",
    "    label=f'mu_hat stderr={intervals.mean(1).std() / np.sqrt(intervals.shape[0]):.3f}',\n",
    ")\n",
    "plt.xlabel('mu')\n",
    "plt.ylabel(f'pseudo experiment seed')\n",
    "# plt.twiny()\n",
    "# plot mu_ideal = sig_observed / sig_expected\n",
    "plt.plot(\n",
    "    [counts['htautau'] / (pseudo_exp_frac * valid_set['weights'][valid_set['labels'] == 1].sum()) for *_, counts in pseudo_results],\n",
    "    [seed for seed, *_ in pseudo_results],\n",
    "    '.',\n",
    "    color='C1',\n",
    "    label='mu_ideal',\n",
    "    # alpha=0.5,\n",
    ")\n",
    "# plot mu_ideal error bars\n",
    "plt.hlines(\n",
    "    [seed for seed, *_ in pseudo_results],\n",
    "    [(counts['htautau'] - np.sqrt(counts['htautau'])) / (pseudo_exp_frac * valid_set['weights'][valid_set['labels'] == 1].sum()) for *_, counts in pseudo_results],\n",
    "    [(counts['htautau'] + np.sqrt(counts['htautau'])) / (pseudo_exp_frac * valid_set['weights'][valid_set['labels'] == 1].sum()) for *_, counts in pseudo_results],\n",
    "    color='C1',\n",
    "    # linestyles='dashed',\n",
    "    # label='p16-p84 poisson',\n",
    "    # alpha=0.5,\n",
    ")\n",
    "# plt.xlim(0, 3)\n",
    "plt.title(f'{num_bins} bins; luminosity fraction={pseudo_exp_frac:.2%}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hist_bins) - 1, (mus[0], mus[-1], len(mus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    (intervals.mean(1) - mu_true) / ((intervals[:, 1] - intervals[:, 0]) / 2), # pulls\n",
    "    bins=20,\n",
    ")\n",
    "plt.xlabel('pull')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in valid_set['data'].columns:\n",
    "    if valid_set['data'][name].dtype != np.float32:\n",
    "        print('Train:', name, valid_set['data'][name].dtype)\n",
    "    if pseudo_exp_data['data'][name].dtype != np.float32:\n",
    "        print('Test: ', name, pseudo_exp_data['data'][name].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_hist(intervals, mu_true=1, bins=100):\n",
    "    pulls = (intervals.mean(1) - mu_true) / ((intervals[:, 1] - intervals[:, 0]) / 2)\n",
    "    return np.histogram(pulls, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((intervals[:, 1] - intervals[:, 0]) / 2, '.')\n",
    "plt.xlabel('pseudo experiment')\n",
    "plt.ylabel('one sigma width')\n",
    "# plt.twinx()\n",
    "# plt.plot(intervals.mean(1), '.', color='r')\n",
    "# plt.ylabel('mu_hat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Score\n",
    "***\n",
    "1. Compute Scores\n",
    "2. Visualize Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_scatter(ingestion_result_dict, ground_truth_mus):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for key in ingestion_result_dict.keys():\n",
    "        ingestion_result = ingestion_result_dict[key]\n",
    "        mu_hat = np.mean(ingestion_result[\"mu_hats\"])\n",
    "        mu = ground_truth_mus[key]\n",
    "        plt.scatter(mu, mu_hat, c='b', marker='o')\n",
    "    \n",
    "    plt.xlabel('Ground Truth $\\mu$')\n",
    "    plt.ylabel('Predicted $\\mu$ (averaged for 100 test sets)')\n",
    "    plt.title('Ground Truth vs. Predicted $\\mu$ Values')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_coverage(ingestion_result_dict, ground_truth_mus):\n",
    "\n",
    "    for key in ingestion_result_dict.keys():\n",
    "        plt.figure( figsize=(5, 5))\n",
    "\n",
    "        ingestion_result = ingestion_result_dict[key]\n",
    "        mu = ground_truth_mus[key]\n",
    "        mu_hats = np.mean(ingestion_result[\"mu_hats\"])\n",
    "        p16s = ingestion_result[\"p16\"]\n",
    "        p84s = ingestion_result[\"p84\"]\n",
    "        \n",
    "        # plot horizontal lines from p16 to p84\n",
    "        for i, (p16, p84) in enumerate(zip(p16s, p84s)):\n",
    "            plt.hlines(y=i, xmin=p16, xmax=p84, colors='b', label='p16-p84')\n",
    "\n",
    "        plt.vlines(x=mu_hats, ymin=0, ymax=len(p16s), colors='r', linestyles='dashed', label='Predicted $\\mu$')\n",
    "        plt.vlines(x=mu, ymin=0, ymax=len(p16s), colors='g', linestyles='dashed', label='Ground Truth $\\mu$')\n",
    "        plt.xlabel('mu')\n",
    "        plt.ylabel('pseudo-experiments')\n",
    "        plt.title(f'mu distribution - Set_{key}')\n",
    "        plt.legend()\n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score import Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Score\n",
    "score = Scoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.load_ingestion_results(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Score\n",
    "score.compute_scores(test_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scatter plot of ground truth mu and predicted mu\n",
    "visualize_scatter(ingestion_result_dict=ingestion.results_dict, \n",
    "                  ground_truth_mus=test_settings[\"ground_truth_mus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coverage\n",
    "visualize_coverage(ingestion_result_dict=ingestion.results_dict, \n",
    "                  ground_truth_mus=test_settings[\"ground_truth_mus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Submission\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prepare the submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from data_io import zipdir\n",
    "the_date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "code_submission = 'HiggsML-code_submission_' + the_date + '.zip'\n",
    "zipdir(code_submission, submission_dir)\n",
    "print(\"Submit : \" + code_submission + \" to the competition\")\n",
    "print(\"You can find the zip file in `HEP-Challenge/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9e001b0608738f9411416229c98988c04b997dc526fb61c5e4e084e768e3249"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
